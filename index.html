<html>
<head>
    <title>Bokun Wang's Home Page</title>
    <style>
  body {background-color: whitesmoke;}
  h1 {color: black;}
  p {color: black;}
.togList
{

}

.togList dt
{
cursor: pointer; cursor: hand;
}

.togList dt span
{

color: blue;
}

.togList dd
{
width: 680px;
padding-bottom: 15px;
}
 								
html.isJS .togList dd
{
display: none;
}
</style>
<script type="text/javascript">

/* Only set closed if JS-enabled */
document.getElementsByTagName('html')[0].className = 'isJS';

function tog(dt)
{
var display, dd=dt;
/* get dd */
do{ dd = dd.nextSibling } while(dd.tagName!='DD');
toOpen =!dd.style.display;
dd.style.display = toOpen? 'block':''
dt.getElementsByTagName('span')[0].innerhtml = toOpen? '-':'+' ;
}
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css"> 
</head>
<body leftmargin="9">
<table class="imgtable" border="0" width="70%" id="AutoNumber1" style="float: left; position: absolute; left: 15%; top:26px">
<body style="font-family:Helvetica;">
  <tr>
    <td width="100%" height="200">
    <table border="0" width="100%" id="AutoNumber2">
      <tr>
        <td width="20%" align="middle"><font face="Arial">
        <img border="0" src="bokun_hd.jpg" width="180" height="180"></font> <br>
        (<a href=https://qiqi-helloworld.github.io/>image credits</a>)
          </td>
        <td width="55%" align="middle">
            <h1><b><font face="Papyrus" color="black"><span style="font-size: 30.0pt">&nbsp;&nbsp;Bokun Wang</b></h1>
        <p class="MsoNormal"><font face="Helvetica"><span lang="EN-US">I am a Ph.D. student under the supervision of <a href="https://homepage.cs.uiowa.edu/~tyng/"><span style="text-decoration: underline; color: blue; margin: 0px">Tianbao Yang</span></a>. I got my bachelor's degree in Computer Science from University of Electronic Science and Technology of China in 2018. My current research interests are stochastic optimization and distributed optimization for machine learning. <br>
         <h1><b><font face="Papyrus" color="black"><span style="font-size: 35.0pt"><a href=mailto:bokunw.wang@gmail.com target=_blank rel=noopener aria-label=envelope><i class="fa fa-envelope-square big-icon"></i></a><a href=https://scholar.google.com/citations?user=H9GqvAYAAAAJ&hl=en target=_blank rel=noopener aria-label=google-scholar>
<i class="ai ai-google-scholar-square big-icon"></i>
</a> <a href=https://github.com/bokunwang1 target=_blank rel=noopener aria-label=github>
<i class="fa fa-github-square big-icon"></i>
</a><a href=""><i class="fa fa-star ai ai-cv-square big-icon"></i></a> </span> 
        </font></b></h1>
        </span></font></td>
      </tr>
    </table>
    <br />
    </td>
    
  </tr>
    
  <tr>
    <td width="100%">
	<h2>
    <font face="Helvetica" color="black"><span style="font-size: 14.0pt"><b>Recent Papers</b> <a href=pub.html><span style="text-decoration: underline; color: blue; margin: 0px">[Full List]</span></a></span></font></h2> 
        <ul>
            <li>
        <b> Finite-Sum Coupled Compositional Stochastic Optimization: Theory and Applications <a href="https://arxiv.org/abs/2202.12396">[arxiv]</a> </b><br>
        Bokun Wang and Tianbao Yang<br> 
        In Proc. of the 39th International Conference on Machine Learning (ICML), 2022. <br>
				<dl class="togList">
          <dt onclick="tog(this)">
            <span>+</span> <font color="blue">abstract</font>
          </dt>
          <dd>  
	   <font face = "Verdana" size = "2" color = #5D6D7E> This paper studies stochastic optimization for a sum of compositional functions, where the inner-level function of each summand is coupled with the corresponding summation index. We refer to this family of problems as finite-sum coupled compositional optimization (FCCO). It has broad applications in machine learning for optimizing non-convex or convex compositional measures/objectives such as average precision (AP), p-norm push, listwise ranking losses, neighborhood component analysis (NCA), deep survival analysis, deep latent variable models, etc., which deserves finer analysis. Yet, existing algorithms and analyses are restricted in one or other aspects. The contribution of this paper is to provide a comprehensive convergence analysis of a simple stochastic algorithm for both non-convex and convex objectives. Our key result is the improved oracle complexity with the parallel speed-up by using the moving-average based estimator with mini-batching. Our theoretical analysis also exhibits new insights for improving the practical implementation by sampling the batches of equal size for the outer and inner levels. Numerical experiments on AP maximization, NCA, and p-norm push corroborate some aspects of the theory.</font>           </dd> 
        </dl></li>
        </ul>  
        
        <ul>
            <li>
        <b> Riemannian Stochastic Proximal Gradient Methods for Nonsmooth Optimization over the Stiefel Manifold <a href="https://www.jmlr.org/papers/volume23/21-0314/21-0314.pdf">[journal]</a> <a href="https://arxiv.org/abs/2005.01209">[arxiv]</a> <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:XJC0PmY54pIJ:scholar.google.com/&output=citation&scisdr=CgXELv9IEImP2iM4nYU:AAGBfm0AAAAAYq0-hYXr3tmnRHqfVh0ZNlzDL0idXBYY&scisig=AAGBfm0AAAAAYq0-hbcx-hzdC6sCnW6r2Y76YIiGzWfV&scisf=4&ct=citation&cd=-1&hl=en&scfhb=1">[bib]</a></b><br>
        Bokun Wang, Shiqian Ma, and Lingzhou Xue<br> 
        In Journal of Machine Learning Research (JMLR), 23(106), 1-33, 2022. <br>
				<dl class="togList">
          <dt onclick="tog(this)">
            <span>+</span> <font color="blue">abstract</font>
          </dt>
          <dd>  
	   <font face = "Verdana" size = "2" color = #5D6D7E>Riemannian optimization has drawn a lot of attention due to its wide applications in practice. Riemannian stochastic first-order algorithms have been studied in the literature to solve large-scale machine learning problems over Riemannian manifolds. However, most of the existing Riemannian stochastic algorithms require the objective function to be differentiable, and they do not apply to the case where the objective function is nonsmooth. In this paper, we present two Riemannian stochastic proximal gradient methods for minimizing nonsmooth function over the Stiefel manifold. The two methods, named R-ProxSGD and R-ProxSPB, are generalizations of proximal SGD and proximal SpiderBoost in Euclidean setting to the Riemannian setting. Analysis on the incremental first-order oracle (IFO) complexity of the proposed algorithms is provided. Specifically, the R-ProxSPB algorithm finds an ϵ-stationary point with O(ϵ^{−3}) IFOs in the online case, and O(n+\sqrt{n}ϵ^{−2}) IFOs in the finite-sum case with n being the number of summands in the objective. Experimental results on online sparse PCA and robust low-rank matrix completion show that our proposed methods significantly outperform the existing methods that use Riemannian subgradient information.</font>          </dd> 
        </dl></li>
        </ul>
        
        <ul>
            <li>
        <b> IntSGD: Adaptive Floatless Compression of Stochastic Gradients <a href="https://openreview.net/forum?id=pFyXqxChZc">[openreview]</a> <a href="https://arxiv.org/abs/2102.08374">[arxiv]</a> <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:-IK4WW0ofusJ:scholar.google.com/&output=citation&scisdr=CgXELv9IEImP2iNHSwM:AAGBfm0AAAAAYq1BUwMtAFU6YdUYbxoEQPr1zDEuhjjB&scisig=AAGBfm0AAAAAYq1BU_qBdqeJeynHaHDhEvNcrQ8Cn9oJ&scisf=4&ct=citation&cd=-1&hl=en&scfhb=1">[bib]</a> <a href="https://github.com/bokunwang1/intsgd">[code]</a> <a href="https://www.konstmish.com/uploads/posters/22-intsgd.pdf">[poster]</a> <a href="https://www.konstmish.com/uploads/slides/22-intsgd.pdf">[slides]</a></b><br>
        Konstantin Mishchenko, Bokun Wang, Dmitry Kovalev, and Peter Richtárik<br> 
        In International Conference on Learning Representations (ICLR), 2022 <font color="red">(Spotlight)</font>. <br>
				<dl class="togList">
          <dt onclick="tog(this)">
            <span>+</span> <font color="blue">abstract</font>
          </dt>
          <dd>  
	   <font face = "Verdana" size = "2" color = #5D6D7E>We propose a family of adaptive integer compression operators for distributed Stochastic Gradient Descent (SGD) that do not communicate a single float. This is achieved by multiplying floating-point vectors with a number known to every device and then rounding to integers. In contrast to the prior work on integer compression for SwitchML by Sapio et al. (2021), our IntSGD method is provably convergent and computationally cheaper as it estimates the scaling of vectors adaptively. Our theory shows that the iteration complexity of IntSGD matches that of SGD up to constant factors for both convex and non-convex, smooth and non-smooth functions, with and without overparameterization. Moreover, our algorithm can also be tailored for the popular all-reduce primitive and shows promising empirical performance.</font>          </dd> 
        </dl></li>
        </ul>	<br>  
</tr>
<tr>
    <td width="100%">
	<h2><b>
    <font face="Helvetica" color="black"><span style="font-size: 14.0pt">Experience</span></font></b></h2>
    <ul>
    <li>
    Optimization and Machine Learning Lab, King Abdullah University of Science and Technology (KAUST)<br> 
    Research intern advised by 
    <a href=https://richtarik.org><span style="text-decoration: underline; color: blue; margin: 0px">Peter Richtárik</span></a>, September 2020 - August 2021.<br> 
    </li>
    </ul>
        
    <ul>
    <li>
    Shenzhen Research Institute of Big Data (SRIBD)<br> 
    Research intern advised by 
    <a href=http://tongzhang-ml.org/><span style="text-decoration: underline; color: blue; margin: 0px">Tong Zhang</span></a>, May 2020 - August 2020.<br> 
    </li>
    </ul>
   
    <ul>
    <li>
    University of California Davis (UC Davis)<br> 
    Research assistant advised by <a href=https://www.math.ucdavis.edu/~sqma/index.html><span style="text-decoration: underline; color: blue; margin: 0px">Shiqian Ma</span></a>, July 2019 - September 2019.<br> 
    </li>
    </ul> 
</tr>
<tr>
    <td width="100%">
	<h2><b>
    <font face="Helvetica" color="black"><span style="font-size: 14.0pt">Teaching</span></font></b></h2>
    <ul>
    <li>
    Teaching assistant of  ECS 32B: Introduction to Data Structures, ECS 154A: Computer Architecture, ECS 271: Machine Learning & Discovery, ECS 170: Introduction to Artificial Intelligence at University of California Davis (UC Davis).
        </li>
    </ul>
    <!--
    <ul>
    <li>
    ECS 32B: Introduction to Data Structures, University of California Davis (UC Davis)<br> 
    Teaching assistant to <a href=https://cs.ucdavis.edu/directory/matt-butner><span style="text-decoration: underline; color: blue; margin: 0px">Matthew Butner</span></a>, Winter 2020.<br> 
    </li>
    </ul>
    <ul>
    <li>
    ECS 154A: Computer Architecture, University of California Davis (UC Davis)<br> 
    Teaching assistant to <a href=https://cs.ucdavis.edu/directory/matt-butner><span style="text-decoration: underline; color: blue; margin: 0px">Matthew Butner</span></a>, Fall 2019.<br> 
    </li>
    </ul>
    <ul>
    <li>
    ECS 271: Machine Learning & Discovery, University of California Davis (UC Davis)<br> 
    Teaching assistant to <a href=https://www.cs.ucdavis.edu/~davidson/><span style="text-decoration: underline; color: blue; margin: 0px">Ian Davidson</span></a>, Spring 2019.<br> 
    </li>
    </ul>
    <ul>
    <li>
    ECS 170: Introduction to Artificial Intelligence, University of California Davis (UC Davis)<br> 
    Teaching assistant to <a href=https://www.cs.ucdavis.edu/~davidson/><span style="text-decoration: underline; color: blue; margin: 0px">Ian Davidson</span></a>, Winter 2019.<br> 
    </li>
    </ul>
-->
</tr>
    <tr>
    <td width="100%">
	<h2><b>
    <font face="Helvetica" color="black"><span style="font-size: 14.0pt">Services</span></font></b></h2>
    <p ALIGN="JUSTIFY"><font face="Helvetica"> Reviewer of NeurIPS, ICML.</font></p>     <br>    
</tr>
</body>
</html>